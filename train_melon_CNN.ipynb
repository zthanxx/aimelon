{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USAGE\n",
    "# python classify_images.py\n",
    "\n",
    "# import the necessary packages\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "models = {\n",
    "\t\"knn\": KNeighborsClassifier(n_neighbors=1),\n",
    "\t\"naive_bayes\": GaussianNB(),\n",
    "\t\"logit\": LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\"),\n",
    "\t\"svm\": SVC(kernel=\"linear\"),\n",
    "\t\"decision_tree\": DecisionTreeClassifier(),\n",
    "\t\"random_forest\": RandomForestClassifier(n_estimators=100),\n",
    "\t\"mlp\": MLPClassifier()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"C:\\Coding\\latihanAI\\DataTraining\\weed\"\n",
    "def extract_color_stats(image):\n",
    "\t# split the input image into its respective RGB color channels\n",
    "\t# and then create a feature vector with 6 values: the mean and\n",
    "\t# standard deviation for each of the 3 channels, respectively\n",
    "\t(R, G, B) = image.split()\n",
    "\tfeatures = [np.mean(R), np.mean(G), np.mean(B), np.std(R),\n",
    "\t\tnp.std(G), np.std(B)]\n",
    "\n",
    "\t# return our set of features\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] extracting image features...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, default=path,\n",
    "\thelp=\"path to directory containing the '3scenes' dataset\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, default=\"knn\",\n",
    "\thelp=\"type of python machine learning model to use\")\n",
    "args = vars( ap.parse_known_args()[0])\n",
    "# define the dictionary of models our script can use, where the key\n",
    "# to the dictionary is the name of the model (supplied via command\n",
    "# line argument) and the value is the model itself\n",
    "\n",
    "# grab all image paths in the input dataset directory, initialize our\n",
    "# list of extracted features and corresponding labels\n",
    "print(\"[INFO] extracting image features...\")\n",
    "imagePaths = paths.list_images(args[\"dataset\"])\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over our input images\n",
    "for imagePath in imagePaths:\n",
    "\t# load the input image from disk, compute color channel\n",
    "\t# statistics, and then update our data list\n",
    "\timage = Image.open(imagePath)\n",
    "\tfeatures = extract_color_stats(image)\n",
    "\tdata.append(features)\n",
    "\n",
    "\t# extract the class label from the file path and update the\n",
    "\t# labels list\n",
    "\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\tlabels.append(label)\n",
    "\n",
    "# encode the labels, converting them from strings to integers\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definisikan Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# perform a training and testing split, using 70% of the data for\n",
    "# training and 30% for evaluation\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "\ttest_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggunakan KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'knn' model\n",
      "[INFO] evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   broadleaf       0.82      0.82      0.82       376\n",
      "       grass       0.73      0.68      0.71       339\n",
      "        soil       0.99      1.00      1.00       349\n",
      "     soybean       0.82      0.86      0.84       370\n",
      "\n",
      "    accuracy                           0.84      1434\n",
      "   macro avg       0.84      0.84      0.84      1434\n",
      "weighted avg       0.84      0.84      0.84      1434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, default=path,\n",
    "\thelp=\"path to directory containing the '3scenes' dataset\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, default=\"knn\",\n",
    "\thelp=\"type of python machine learning model to use\")\n",
    "args = vars( ap.parse_known_args()[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(args[\"model\"]))\n",
    "model = models[args[\"model\"]]\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating...\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggunakan Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'naive_bayes' model\n",
      "[INFO] evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   broadleaf       0.55      0.76      0.64       376\n",
      "       grass       0.56      0.35      0.43       339\n",
      "        soil       0.92      0.94      0.93       349\n",
      "     soybean       0.60      0.56      0.58       370\n",
      "\n",
      "    accuracy                           0.66      1434\n",
      "   macro avg       0.66      0.65      0.65      1434\n",
      "weighted avg       0.66      0.66      0.65      1434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, default=path,\n",
    "\thelp=\"path to directory containing the '3scenes' dataset\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, default=\"naive_bayes\",\n",
    "\thelp=\"type of python machine learning model to use\")\n",
    "args = vars( ap.parse_known_args()[0])\n",
    "\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(args[\"model\"]))\n",
    "model = models[args[\"model\"]]\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating...\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggunakan Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'logit' model\n",
      "[INFO] evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   broadleaf       0.69      0.69      0.69       376\n",
      "       grass       0.55      0.50      0.52       339\n",
      "        soil       0.99      1.00      1.00       349\n",
      "     soybean       0.68      0.74      0.71       370\n",
      "\n",
      "    accuracy                           0.73      1434\n",
      "   macro avg       0.73      0.73      0.73      1434\n",
      "weighted avg       0.73      0.73      0.73      1434\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zthan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, default=path,\n",
    "\thelp=\"path to directory containing the '3scenes' dataset\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, default=\"logit\",\n",
    "\thelp=\"type of python machine learning model to use\")\n",
    "args = vars( ap.parse_known_args()[0])\n",
    "\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(args[\"model\"]))\n",
    "model = models[args[\"model\"]]\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating...\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggunakan SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'svm' model\n",
      "[INFO] evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   broadleaf       0.71      0.71      0.71       376\n",
      "       grass       0.60      0.56      0.58       339\n",
      "        soil       0.99      1.00      0.99       349\n",
      "     soybean       0.71      0.74      0.72       370\n",
      "\n",
      "    accuracy                           0.75      1434\n",
      "   macro avg       0.75      0.75      0.75      1434\n",
      "weighted avg       0.75      0.75      0.75      1434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, default=path,\n",
    "\thelp=\"path to directory containing the '3scenes' dataset\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, default=\"svm\",\n",
    "\thelp=\"type of python machine learning model to use\")\n",
    "args = vars( ap.parse_known_args()[0])\n",
    "\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(args[\"model\"]))\n",
    "model = models[args[\"model\"]]\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating...\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggunakan Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'decision_tree' model\n",
      "[INFO] evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   broadleaf       0.69      0.72      0.70       303\n",
      "       grass       0.73      0.67      0.70       486\n",
      "        soil       0.98      0.98      0.98       532\n",
      "     soybean       0.85      0.88      0.87       606\n",
      "\n",
      "    accuracy                           0.83      1927\n",
      "   macro avg       0.81      0.81      0.81      1927\n",
      "weighted avg       0.83      0.83      0.83      1927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, default=path,\n",
    "\thelp=\"path to directory containing the '3scenes' dataset\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, default=\"decision_tree\",\n",
    "\thelp=\"type of python machine learning model to use\")\n",
    "args = vars( ap.parse_known_args()[0])\n",
    "\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(args[\"model\"]))\n",
    "model = models[args[\"model\"]]\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating...\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggunakan Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'random_forest' model\n",
      "[INFO] evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   broadleaf       0.86      0.84      0.85       376\n",
      "       grass       0.75      0.74      0.74       339\n",
      "        soil       0.99      1.00      0.99       349\n",
      "     soybean       0.85      0.87      0.86       370\n",
      "\n",
      "    accuracy                           0.86      1434\n",
      "   macro avg       0.86      0.86      0.86      1434\n",
      "weighted avg       0.86      0.86      0.86      1434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, default=path,\n",
    "\thelp=\"path to directory containing the '3scenes' dataset\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, default=\"random_forest\",\n",
    "\thelp=\"type of python machine learning model to use\")\n",
    "args = vars( ap.parse_known_args()[0])\n",
    "\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(args[\"model\"]))\n",
    "model = models[args[\"model\"]]\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating...\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menggunakan Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using 'mlp' model\n",
      "[INFO] evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   broadleaf       0.87      0.83      0.85       376\n",
      "       grass       0.75      0.76      0.75       339\n",
      "        soil       0.99      1.00      1.00       349\n",
      "     soybean       0.85      0.87      0.86       370\n",
      "\n",
      "    accuracy                           0.86      1434\n",
      "   macro avg       0.86      0.86      0.86      1434\n",
      "weighted avg       0.86      0.86      0.86      1434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", type=str, default=path,\n",
    "\thelp=\"path to directory containing the '3scenes' dataset\")\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, default=\"mlp\",\n",
    "\thelp=\"type of python machine learning model to use\")\n",
    "args = vars( ap.parse_known_args()[0])\n",
    "\n",
    "\n",
    "# train the model\n",
    "print(\"[INFO] using '{}' model\".format(args[\"model\"]))\n",
    "model = models[args[\"model\"]]\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# make predictions on our data and show a classification report\n",
    "print(\"[INFO] evaluating...\")\n",
    "predictions = model.predict(testX)\n",
    "print(classification_report(testY, predictions,\n",
    "\ttarget_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menggunakan CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CNN dengan dimensi 32x32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weed_model_1dim_32x32.sav\n",
      "5\n",
      "[INFO] loading images...\n",
      "[INFO] training network...\n",
      "Train on 4302 samples, validate on 478 samples\n",
      "Epoch 1/50\n",
      "4302/4302 [==============================] - 7s 2ms/step - loss: 0.8534 - acc: 0.5853 - val_loss: 0.6437 - val_acc: 0.6506\n",
      "Epoch 2/50\n",
      "4302/4302 [==============================] - 3s 586us/step - loss: 0.5941 - acc: 0.7157 - val_loss: 0.5056 - val_acc: 0.7992\n",
      "Epoch 3/50\n",
      "4302/4302 [==============================] - 3s 603us/step - loss: 0.5338 - acc: 0.7585 - val_loss: 0.4638 - val_acc: 0.8180\n",
      "Epoch 4/50\n",
      "4302/4302 [==============================] - 3s 584us/step - loss: 0.4935 - acc: 0.7820 - val_loss: 0.4030 - val_acc: 0.8326\n",
      "Epoch 5/50\n",
      "4302/4302 [==============================] - 2s 560us/step - loss: 0.4511 - acc: 0.8110 - val_loss: 0.3853 - val_acc: 0.8619\n",
      "Epoch 6/50\n",
      "4302/4302 [==============================] - 2s 559us/step - loss: 0.4264 - acc: 0.8243 - val_loss: 0.3365 - val_acc: 0.8661\n",
      "Epoch 7/50\n",
      "4302/4302 [==============================] - 2s 562us/step - loss: 0.3938 - acc: 0.8408 - val_loss: 0.3563 - val_acc: 0.8556\n",
      "Epoch 8/50\n",
      "4302/4302 [==============================] - 2s 559us/step - loss: 0.3797 - acc: 0.8466 - val_loss: 0.3145 - val_acc: 0.8745\n",
      "Epoch 9/50\n",
      "4302/4302 [==============================] - 2s 572us/step - loss: 0.3641 - acc: 0.8529 - val_loss: 0.4083 - val_acc: 0.8368\n",
      "Epoch 10/50\n",
      "4302/4302 [==============================] - 3s 627us/step - loss: 0.3532 - acc: 0.8547 - val_loss: 0.2894 - val_acc: 0.9079\n",
      "Epoch 11/50\n",
      "4302/4302 [==============================] - 2s 566us/step - loss: 0.3239 - acc: 0.8722 - val_loss: 0.2787 - val_acc: 0.8912\n",
      "Epoch 12/50\n",
      "4302/4302 [==============================] - 2s 560us/step - loss: 0.3271 - acc: 0.8724 - val_loss: 0.3069 - val_acc: 0.8828\n",
      "Epoch 13/50\n",
      "4302/4302 [==============================] - 2s 563us/step - loss: 0.3579 - acc: 0.8545 - val_loss: 0.2878 - val_acc: 0.8954\n",
      "Epoch 14/50\n",
      "4302/4302 [==============================] - 2s 561us/step - loss: 0.3147 - acc: 0.8747 - val_loss: 0.3448 - val_acc: 0.8703\n",
      "Epoch 15/50\n",
      "4302/4302 [==============================] - 2s 567us/step - loss: 0.3141 - acc: 0.8749 - val_loss: 0.2776 - val_acc: 0.8954\n",
      "Epoch 16/50\n",
      "4302/4302 [==============================] - 3s 596us/step - loss: 0.2963 - acc: 0.8840 - val_loss: 0.2956 - val_acc: 0.8849\n",
      "Epoch 17/50\n",
      "4302/4302 [==============================] - 2s 581us/step - loss: 0.2829 - acc: 0.8877 - val_loss: 0.2674 - val_acc: 0.9017\n",
      "Epoch 18/50\n",
      "4302/4302 [==============================] - 2s 571us/step - loss: 0.2805 - acc: 0.8907 - val_loss: 0.2613 - val_acc: 0.8996\n",
      "Epoch 19/50\n",
      "4302/4302 [==============================] - 2s 572us/step - loss: 0.2853 - acc: 0.8903 - val_loss: 0.2613 - val_acc: 0.9038\n",
      "Epoch 20/50\n",
      "4302/4302 [==============================] - 2s 568us/step - loss: 0.2817 - acc: 0.8949 - val_loss: 0.2948 - val_acc: 0.8891\n",
      "Epoch 21/50\n",
      "4302/4302 [==============================] - 2s 567us/step - loss: 0.2721 - acc: 0.8956 - val_loss: 0.2534 - val_acc: 0.8996\n",
      "Epoch 22/50\n",
      "4302/4302 [==============================] - 3s 604us/step - loss: 0.2636 - acc: 0.8949 - val_loss: 0.2570 - val_acc: 0.8996\n",
      "Epoch 23/50\n",
      "4302/4302 [==============================] - 3s 619us/step - loss: 0.2516 - acc: 0.9042 - val_loss: 0.2617 - val_acc: 0.9059\n",
      "Epoch 24/50\n",
      "4302/4302 [==============================] - 3s 600us/step - loss: 0.2531 - acc: 0.9047 - val_loss: 0.2573 - val_acc: 0.9163\n",
      "Epoch 25/50\n",
      "4302/4302 [==============================] - 3s 592us/step - loss: 0.2430 - acc: 0.9084 - val_loss: 0.2844 - val_acc: 0.8954\n",
      "Epoch 26/50\n",
      "4302/4302 [==============================] - 2s 570us/step - loss: 0.2428 - acc: 0.9098 - val_loss: 0.2986 - val_acc: 0.8933\n",
      "Epoch 27/50\n",
      "4302/4302 [==============================] - 2s 562us/step - loss: 0.2315 - acc: 0.9114 - val_loss: 0.2385 - val_acc: 0.9163\n",
      "Epoch 28/50\n",
      "4302/4302 [==============================] - 2s 561us/step - loss: 0.2100 - acc: 0.9228 - val_loss: 0.2256 - val_acc: 0.9205\n",
      "Epoch 29/50\n",
      "4302/4302 [==============================] - 3s 612us/step - loss: 0.2121 - acc: 0.9233 - val_loss: 0.2830 - val_acc: 0.9142\n",
      "Epoch 30/50\n",
      "4302/4302 [==============================] - 2s 577us/step - loss: 0.2116 - acc: 0.9198 - val_loss: 0.2269 - val_acc: 0.9331\n",
      "Epoch 31/50\n",
      "4302/4302 [==============================] - 2s 570us/step - loss: 0.1978 - acc: 0.9291 - val_loss: 0.2203 - val_acc: 0.9310\n",
      "Epoch 32/50\n",
      "4302/4302 [==============================] - 2s 569us/step - loss: 0.2009 - acc: 0.9268 - val_loss: 0.2202 - val_acc: 0.9205\n",
      "Epoch 33/50\n",
      "4302/4302 [==============================] - 2s 567us/step - loss: 0.1764 - acc: 0.9393 - val_loss: 0.2483 - val_acc: 0.9079\n",
      "Epoch 34/50\n",
      "4302/4302 [==============================] - 2s 576us/step - loss: 0.1806 - acc: 0.9368 - val_loss: 0.3308 - val_acc: 0.8766\n",
      "Epoch 35/50\n",
      "4302/4302 [==============================] - 3s 586us/step - loss: 0.1777 - acc: 0.9354 - val_loss: 0.2386 - val_acc: 0.9142\n",
      "Epoch 36/50\n",
      "4302/4302 [==============================] - 3s 591us/step - loss: 0.1688 - acc: 0.9405 - val_loss: 0.2504 - val_acc: 0.9059\n",
      "Epoch 37/50\n",
      "4302/4302 [==============================] - 2s 570us/step - loss: 0.1731 - acc: 0.9379 - val_loss: 0.2685 - val_acc: 0.8996\n",
      "Epoch 38/50\n",
      "4302/4302 [==============================] - 2s 545us/step - loss: 0.1516 - acc: 0.9486 - val_loss: 0.2542 - val_acc: 0.9100\n",
      "Epoch 39/50\n",
      "4302/4302 [==============================] - 2s 562us/step - loss: 0.1495 - acc: 0.9489 - val_loss: 0.2431 - val_acc: 0.9226\n",
      "Epoch 40/50\n",
      "4302/4302 [==============================] - 2s 548us/step - loss: 0.1745 - acc: 0.9375 - val_loss: 0.3258 - val_acc: 0.8891\n",
      "Epoch 41/50\n",
      "4302/4302 [==============================] - 2s 552us/step - loss: 0.1457 - acc: 0.9505 - val_loss: 0.2451 - val_acc: 0.9226\n",
      "Epoch 42/50\n",
      "4302/4302 [==============================] - 3s 632us/step - loss: 0.1298 - acc: 0.9575 - val_loss: 0.2296 - val_acc: 0.9247\n",
      "Epoch 43/50\n",
      "4302/4302 [==============================] - 2s 576us/step - loss: 0.1500 - acc: 0.9505 - val_loss: 0.3367 - val_acc: 0.8766\n",
      "Epoch 44/50\n",
      "4302/4302 [==============================] - 2s 549us/step - loss: 0.1321 - acc: 0.9579 - val_loss: 0.2505 - val_acc: 0.9226\n",
      "Epoch 45/50\n",
      "4302/4302 [==============================] - 2s 545us/step - loss: 0.1194 - acc: 0.9614 - val_loss: 0.2422 - val_acc: 0.9226\n",
      "Epoch 46/50\n",
      "4302/4302 [==============================] - 2s 549us/step - loss: 0.1273 - acc: 0.9577 - val_loss: 0.3200 - val_acc: 0.8849\n",
      "Epoch 47/50\n",
      "4302/4302 [==============================] - 2s 540us/step - loss: 0.1281 - acc: 0.9577 - val_loss: 0.2425 - val_acc: 0.9268\n",
      "Epoch 48/50\n",
      "4302/4302 [==============================] - 2s 571us/step - loss: 0.1140 - acc: 0.9593 - val_loss: 0.2445 - val_acc: 0.9247\n",
      "Epoch 49/50\n",
      "4302/4302 [==============================] - 3s 665us/step - loss: 0.1062 - acc: 0.9644 - val_loss: 0.2776 - val_acc: 0.9038\n",
      "Epoch 50/50\n",
      "4302/4302 [==============================] - 3s 598us/step - loss: 0.0960 - acc: 0.9719 - val_loss: 0.2461 - val_acc: 0.9205\n",
      "[INFO] Saving Model...\n",
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   broadleaf       0.91      0.80      0.85       121\n",
      "       grass       0.85      0.91      0.88       116\n",
      "        soil       1.00      1.00      1.00       127\n",
      "     soybean       0.92      0.97      0.94       114\n",
      "\n",
      "    accuracy                           0.92       478\n",
      "   macro avg       0.92      0.92      0.92       478\n",
      "weighted avg       0.92      0.92      0.92       478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python basic_cnn.py\n",
    "\n",
    "# import the necessary packages\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import os.path  \n",
    "import pickle\n",
    "import cv2\n",
    "path = \"C:\\Coding\\latihanAI\\DataTraining\\weed\"\n",
    "BS = 32\n",
    "IMAGE_DIMS = (32, 32 ,3)\n",
    "model_name = 'weed_model_1'+'dim_'+str(IMAGE_DIMS[0])+'x'+str(IMAGE_DIMS[1])+'.sav'\n",
    "print(model_name)\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for name in dirs:\n",
    "        \n",
    "        count += 1\n",
    "print(count)\n",
    "\n",
    "\n",
    "# grab all image paths in the input dataset directory, then initialize\n",
    "# our list of images and corresponding class labels\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = paths.list_images(path)\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "# loop over our input images\n",
    "for imagePath in imagePaths:\n",
    "\t# load the input image from disk, resize it to 32x32 pixels, scale\n",
    "\t# the pixel intensities to the range [0, 1], and then update our\n",
    "\t# images list\n",
    "\timage = cv2.imread(imagePath)\n",
    "\timage = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "\timage = img_to_array(image)\n",
    "\tdata.append(image)\n",
    "\n",
    "\t# extract the class label from the file path and update the\n",
    "\t# labels list\n",
    "\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\tlabels.append(label)\n",
    "\t\n",
    "\n",
    "# encode the labels, converting them from strings to integers\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "# perform a training and testing split, using 70% of the data for\n",
    "# training and 30% for evaluation\n",
    "(trainX, testX, trainY, testY) = train_test_split(np.array(data),\n",
    "\tnp.array(labels), test_size=0.3, random_state=3)\n",
    "\n",
    "# define our Convolutional Neural Network architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, (3, 3), padding=\"same\", input_shape=IMAGE_DIMS))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(count-1))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# train the model using the Adam optimizer\n",
    "print(\"[INFO] training network...\")\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 50)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "\tepochs=50, batch_size=BS)\n",
    "\n",
    "print(\"[INFO] Saving Model...\")\n",
    "filename = model_name\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=lb.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image ukuran 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weed_model_dim_64x64.sav\n",
      "5\n",
      "[INFO] loading images...\n",
      "[INFO] training network...\n",
      "Train on 3346 samples, validate on 1434 samples\n",
      "Epoch 1/50\n",
      "3346/3346 [==============================] - 15s 5ms/step - loss: 1.0076 - acc: 0.5478 - val_loss: 0.6360 - val_acc: 0.6715\n",
      "Epoch 2/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.6172 - acc: 0.7089 - val_loss: 0.5836 - val_acc: 0.7155\n",
      "Epoch 3/50\n",
      "3346/3346 [==============================] - 8s 2ms/step - loss: 0.5504 - acc: 0.7379 - val_loss: 0.5878 - val_acc: 0.7406\n",
      "Epoch 4/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.5211 - acc: 0.7705 - val_loss: 0.4896 - val_acc: 0.7866\n",
      "Epoch 5/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.4664 - acc: 0.8162 - val_loss: 0.4491 - val_acc: 0.8152\n",
      "Epoch 6/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.4331 - acc: 0.8276 - val_loss: 0.4822 - val_acc: 0.8159\n",
      "Epoch 7/50\n",
      "3346/3346 [==============================] - 8s 2ms/step - loss: 0.4154 - acc: 0.8335 - val_loss: 0.3999 - val_acc: 0.8403\n",
      "Epoch 8/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.3919 - acc: 0.8500 - val_loss: 0.3962 - val_acc: 0.8508\n",
      "Epoch 9/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.3697 - acc: 0.8577 - val_loss: 0.3632 - val_acc: 0.8508\n",
      "Epoch 10/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.3495 - acc: 0.8631 - val_loss: 0.4522 - val_acc: 0.8312\n",
      "Epoch 11/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.3379 - acc: 0.8703 - val_loss: 0.3492 - val_acc: 0.8675\n",
      "Epoch 12/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.3024 - acc: 0.8805 - val_loss: 0.4255 - val_acc: 0.8243\n",
      "Epoch 13/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.2965 - acc: 0.8861 - val_loss: 0.3367 - val_acc: 0.8731\n",
      "Epoch 14/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.2794 - acc: 0.8954 - val_loss: 0.3137 - val_acc: 0.8849\n",
      "Epoch 15/50\n",
      "3346/3346 [==============================] - 6s 2ms/step - loss: 0.2656 - acc: 0.8993 - val_loss: 0.3190 - val_acc: 0.8703\n",
      "Epoch 16/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.2513 - acc: 0.9014 - val_loss: 0.3163 - val_acc: 0.8759\n",
      "Epoch 17/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.2551 - acc: 0.9071 - val_loss: 0.3022 - val_acc: 0.8787\n",
      "Epoch 18/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.2401 - acc: 0.9085 - val_loss: 0.2893 - val_acc: 0.8891\n",
      "Epoch 19/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.2373 - acc: 0.9112 - val_loss: 0.4389 - val_acc: 0.8166\n",
      "Epoch 20/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.2477 - acc: 0.9068 - val_loss: 0.3020 - val_acc: 0.8870\n",
      "Epoch 21/50\n",
      "3346/3346 [==============================] - 6s 2ms/step - loss: 0.2069 - acc: 0.9241 - val_loss: 0.2570 - val_acc: 0.9038\n",
      "Epoch 22/50\n",
      "3346/3346 [==============================] - 6s 2ms/step - loss: 0.1890 - acc: 0.9328 - val_loss: 0.2770 - val_acc: 0.8947\n",
      "Epoch 23/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.2348 - acc: 0.9097 - val_loss: 0.2724 - val_acc: 0.8926\n",
      "Epoch 24/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.2015 - acc: 0.9283 - val_loss: 0.2661 - val_acc: 0.9031\n",
      "Epoch 25/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1928 - acc: 0.9301 - val_loss: 0.2994 - val_acc: 0.8898\n",
      "Epoch 26/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1783 - acc: 0.9351 - val_loss: 0.2553 - val_acc: 0.9073\n",
      "Epoch 27/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1856 - acc: 0.9238 - val_loss: 0.2812 - val_acc: 0.8975\n",
      "Epoch 28/50\n",
      "3346/3346 [==============================] - 8s 2ms/step - loss: 0.1833 - acc: 0.9322 - val_loss: 0.2538 - val_acc: 0.9066\n",
      "Epoch 29/50\n",
      "3346/3346 [==============================] - 8s 2ms/step - loss: 0.1570 - acc: 0.9447 - val_loss: 0.2587 - val_acc: 0.9073\n",
      "Epoch 30/50\n",
      "3346/3346 [==============================] - 8s 2ms/step - loss: 0.1580 - acc: 0.9459 - val_loss: 0.2421 - val_acc: 0.9149\n",
      "Epoch 31/50\n",
      "3346/3346 [==============================] - 8s 2ms/step - loss: 0.1467 - acc: 0.9474 - val_loss: 0.2925 - val_acc: 0.8898\n",
      "Epoch 32/50\n",
      "3346/3346 [==============================] - 8s 2ms/step - loss: 0.1499 - acc: 0.9426 - val_loss: 0.2465 - val_acc: 0.9149\n",
      "Epoch 33/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1436 - acc: 0.9456 - val_loss: 0.2771 - val_acc: 0.9031\n",
      "Epoch 34/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1314 - acc: 0.9537 - val_loss: 0.3171 - val_acc: 0.8905\n",
      "Epoch 35/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1528 - acc: 0.9411 - val_loss: 0.2828 - val_acc: 0.8968\n",
      "Epoch 36/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1193 - acc: 0.9588 - val_loss: 0.2646 - val_acc: 0.9086\n",
      "Epoch 37/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1373 - acc: 0.9468 - val_loss: 0.2656 - val_acc: 0.9010\n",
      "Epoch 38/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1268 - acc: 0.9513 - val_loss: 0.2948 - val_acc: 0.8912\n",
      "Epoch 39/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1193 - acc: 0.9558 - val_loss: 0.3498 - val_acc: 0.8703\n",
      "Epoch 40/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1226 - acc: 0.9546 - val_loss: 0.2656 - val_acc: 0.9100\n",
      "Epoch 41/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1031 - acc: 0.9659 - val_loss: 0.2859 - val_acc: 0.9024\n",
      "Epoch 42/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1110 - acc: 0.9635 - val_loss: 0.2697 - val_acc: 0.9135\n",
      "Epoch 43/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1219 - acc: 0.9585 - val_loss: 0.3096 - val_acc: 0.8954\n",
      "Epoch 44/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.0994 - acc: 0.9647 - val_loss: 0.2685 - val_acc: 0.9045\n",
      "Epoch 45/50\n",
      "3346/3346 [==============================] - 8s 2ms/step - loss: 0.0890 - acc: 0.9707 - val_loss: 0.3341 - val_acc: 0.8863\n",
      "Epoch 46/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.0924 - acc: 0.9707 - val_loss: 0.2780 - val_acc: 0.9031\n",
      "Epoch 47/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.0803 - acc: 0.9746 - val_loss: 0.2773 - val_acc: 0.9107\n",
      "Epoch 48/50\n",
      "3346/3346 [==============================] - 8s 2ms/step - loss: 0.0678 - acc: 0.9833 - val_loss: 0.2956 - val_acc: 0.9066\n",
      "Epoch 49/50\n",
      "3346/3346 [==============================] - 8s 2ms/step - loss: 0.1403 - acc: 0.9426 - val_loss: 0.5523 - val_acc: 0.8410\n",
      "Epoch 50/50\n",
      "3346/3346 [==============================] - 7s 2ms/step - loss: 0.1061 - acc: 0.9659 - val_loss: 0.2930 - val_acc: 0.9031\n",
      "[INFO] Saving Model...\n",
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   broadleaf       0.88      0.83      0.85       363\n",
      "       grass       0.78      0.87      0.82       335\n",
      "        soil       1.00      1.00      1.00       373\n",
      "     soybean       0.96      0.91      0.94       363\n",
      "\n",
      "    accuracy                           0.90      1434\n",
      "   macro avg       0.90      0.90      0.90      1434\n",
      "weighted avg       0.91      0.90      0.90      1434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python basic_cnn.py\n",
    "\n",
    "# import the necessary packages\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import os.path  \n",
    "import pickle\n",
    "import cv2\n",
    "path = \"C:\\Coding\\latihanAI\\DataTraining\\weed\"\n",
    "BS = 64\n",
    "IMAGE_DIMS = (64, 64 ,3)\n",
    "model_name = 'weed_model_'+'dim_'+str(IMAGE_DIMS[0])+'x'+str(IMAGE_DIMS[1])+'.sav'\n",
    "print(model_name)\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for name in dirs:\n",
    "        \n",
    "        count += 1\n",
    "print(count)\n",
    "\n",
    "\n",
    "# grab all image paths in the input dataset directory, then initialize\n",
    "# our list of images and corresponding class labels\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = paths.list_images(path)\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "# loop over our input images\n",
    "for imagePath in imagePaths:\n",
    "\t# load the input image from disk, resize it to 32x32 pixels, scale\n",
    "\t# the pixel intensities to the range [0, 1], and then update our\n",
    "\t# images list\n",
    "\timage = cv2.imread(imagePath)\n",
    "\timage = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "\timage = img_to_array(image)\n",
    "\tdata.append(image)\n",
    "\n",
    "\t# extract the class label from the file path and update the\n",
    "\t# labels list\n",
    "\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\tlabels.append(label)\n",
    "\t\n",
    "\n",
    "# encode the labels, converting them from strings to integers\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "# perform a training and testing split, using 70% of the data for\n",
    "# training and 30% for evaluation\n",
    "(trainX, testX, trainY, testY) = train_test_split(np.array(data),\n",
    "\tnp.array(labels), test_size=0.3, random_state=3)\n",
    "\n",
    "# define our Convolutional Neural Network architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, (3, 3), padding=\"same\", input_shape=IMAGE_DIMS))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(count-1))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# train the model using the Adam optimizer\n",
    "print(\"[INFO] training network...\")\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 50)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "\tepochs=50, batch_size=BS)\n",
    "\n",
    "print(\"[INFO] Saving Model...\")\n",
    "filename = model_name\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=lb.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN LeNet Architecture 32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weed_model_CNN_LeNet_dim_32x32.sav\n",
      "2\n",
      "[INFO] loading images...\n",
      "[INFO] training network...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1930, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5283, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 2) vs (None, 1)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f300cf92ebf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     89\u001b[0m model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n\u001b[0;32m     90\u001b[0m \tmetrics=[\"binary_accuracy\"])\n\u001b[1;32m---> 91\u001b[1;33m H = model.fit(trainX, trainY, validation_data=(testX, testY),\n\u001b[0m\u001b[0;32m     92\u001b[0m \tepochs=20, batch_size=BS)\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 1930, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"c:\\Users\\zthan\\anaconda3\\lib\\site-packages\\keras\\backend.py\", line 5283, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 2) vs (None, 1)).\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import os.path  \n",
    "import pickle\n",
    "import cv2\n",
    "path = \"D:\\coding\\AI_code\\keras\\melon\\dataset\"\n",
    "BS =32 \n",
    "IMAGE_DIMS = (32, 32 ,3)\n",
    "model_name = 'weed_model_CNN_LeNet_'+'dim_'+str(IMAGE_DIMS[0])+'x'+str(IMAGE_DIMS[1])+'.sav'\n",
    "print(model_name)\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for name in dirs:\n",
    "        \n",
    "        count += 1\n",
    "print(count)\n",
    "\n",
    "\n",
    "# grab all image paths in the input dataset directory, then initialize\n",
    "# our list of images and corresponding class labels\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = paths.list_images(path)\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "# loop over our input images\n",
    "for imagePath in imagePaths:\n",
    "\t# load the input image from disk, resize it to 32x32 pixels, scale\n",
    "\t# the pixel intensities to the range [0, 1], and then update our\n",
    "\t# images list\n",
    "\timage = cv2.imread(imagePath)\n",
    "\timage = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "\timage = img_to_array(image)\n",
    "\tdata.append(image)\n",
    "\n",
    "\t# extract the class label from the file path and update the\n",
    "\t# labels list\n",
    "\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\tlabels.append(label)\n",
    "\t\n",
    "\n",
    "# encode the labels, converting them from strings to integers\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "# perform a training and testing split, using 70% of the data for\n",
    "# training and 30% for evaluation\n",
    "(trainX, testX, trainY, testY) = train_test_split(np.array(data),\n",
    "\tnp.array(labels), test_size=0.3, random_state=3)\n",
    "\n",
    "# first set of CONV => RELU => POOL layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(20, (5, 5), padding=\"same\",input_shape=IMAGE_DIMS))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# second set of CONV => RELU => POOL layers\n",
    "model.add(Conv2D(50, (5, 5), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# first (and only) set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation(\"relu\"))\n",
    "# softmax classifier\n",
    "model.add(Dense(count))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# train the model using the Adam optimizer\n",
    "print(\"[INFO] training network...\")\n",
    "opt = Adam(learning_rate=1e-3, decay=1e-3 / 50)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"binary_accuracy\"])\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "\tepochs=20, batch_size=BS)\n",
    "\n",
    "print(\"[INFO] Saving Model...\")\n",
    "filename = model_name\n",
    "model.save(filename)\n",
    "\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=lb.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN LeNet Architecture 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weed_model_CNN_LeNet_dim_64x64.sav\n",
      "5\n",
      "[INFO] loading images...\n",
      "[INFO] training network...\n",
      "Train on 3346 samples, validate on 1434 samples\n",
      "Epoch 1/20\n",
      "3346/3346 [==============================] - 94s 28ms/step - loss: 0.9837 - acc: 0.5403 - val_loss: 0.7165 - val_acc: 0.6660\n",
      "Epoch 2/20\n",
      "3346/3346 [==============================] - 97s 29ms/step - loss: 0.5999 - acc: 0.7259 - val_loss: 0.5175 - val_acc: 0.7678\n",
      "Epoch 3/20\n",
      "3346/3346 [==============================] - 95s 28ms/step - loss: 0.4475 - acc: 0.8156 - val_loss: 0.4493 - val_acc: 0.8006\n",
      "Epoch 4/20\n",
      "3346/3346 [==============================] - 83s 25ms/step - loss: 0.3453 - acc: 0.8634 - val_loss: 0.3674 - val_acc: 0.8556\n",
      "Epoch 5/20\n",
      "3346/3346 [==============================] - 81s 24ms/step - loss: 0.2866 - acc: 0.8909 - val_loss: 0.5487 - val_acc: 0.7741\n",
      "Epoch 6/20\n",
      "3346/3346 [==============================] - 82s 24ms/step - loss: 0.2471 - acc: 0.9062 - val_loss: 0.3340 - val_acc: 0.8668\n",
      "Epoch 7/20\n",
      "3346/3346 [==============================] - 88s 26ms/step - loss: 0.1984 - acc: 0.9277 - val_loss: 0.4041 - val_acc: 0.8466\n",
      "Epoch 8/20\n",
      "3346/3346 [==============================] - 88s 26ms/step - loss: 0.1104 - acc: 0.9665 - val_loss: 0.3932 - val_acc: 0.8696\n",
      "Epoch 9/20\n",
      "3346/3346 [==============================] - 83s 25ms/step - loss: 0.2708 - acc: 0.9091 - val_loss: 0.4535 - val_acc: 0.8354\n",
      "Epoch 10/20\n",
      "3346/3346 [==============================] - 96s 29ms/step - loss: 0.0866 - acc: 0.9734 - val_loss: 0.4517 - val_acc: 0.8466\n",
      "Epoch 11/20\n",
      "3346/3346 [==============================] - 103s 31ms/step - loss: 0.0385 - acc: 0.9925 - val_loss: 0.4110 - val_acc: 0.8703\n",
      "Epoch 12/20\n",
      "3346/3346 [==============================] - 101s 30ms/step - loss: 0.0176 - acc: 0.9979 - val_loss: 0.5022 - val_acc: 0.8550\n",
      "Epoch 13/20\n",
      "3346/3346 [==============================] - 96s 29ms/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.4578 - val_acc: 0.8703\n",
      "Epoch 14/20\n",
      "3346/3346 [==============================] - 92s 27ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.4994 - val_acc: 0.8724\n",
      "Epoch 15/20\n",
      "3346/3346 [==============================] - 96s 29ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.4937 - val_acc: 0.8759\n",
      "Epoch 16/20\n",
      "3346/3346 [==============================] - 114s 34ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.5111 - val_acc: 0.8724\n",
      "Epoch 17/20\n",
      "3346/3346 [==============================] - 104s 31ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.5458 - val_acc: 0.8661\n",
      "Epoch 18/20\n",
      "3346/3346 [==============================] - 113s 34ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.5294 - val_acc: 0.8780\n",
      "Epoch 19/20\n",
      "3346/3346 [==============================] - 110s 33ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5371 - val_acc: 0.8801\n",
      "Epoch 20/20\n",
      "3346/3346 [==============================] - 95s 28ms/step - loss: 8.9228e-04 - acc: 1.0000 - val_loss: 0.5465 - val_acc: 0.8759\n",
      "[INFO] Saving Model...\n",
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   broadleaf       0.82      0.81      0.81       363\n",
      "       grass       0.77      0.82      0.79       335\n",
      "        soil       0.99      0.99      0.99       373\n",
      "     soybean       0.92      0.88      0.90       363\n",
      "\n",
      "    accuracy                           0.88      1434\n",
      "   macro avg       0.88      0.87      0.87      1434\n",
      "weighted avg       0.88      0.88      0.88      1434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from PIL import Image\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import os.path  \n",
    "import pickle\n",
    "import cv2\n",
    "path = \"D:\\coding\\AI_code\\keras\\dataset\"\n",
    "BS =64\n",
    "IMAGE_DIMS = (64, 64 ,3)\n",
    "model_name = 'weed_model_CNN_LeNet_'+'dim_'+str(IMAGE_DIMS[0])+'x'+str(IMAGE_DIMS[1])+'.sav'\n",
    "print(model_name)\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for name in dirs:\n",
    "        \n",
    "        count += 1\n",
    "print(count)\n",
    "\n",
    "\n",
    "# grab all image paths in the input dataset directory, then initialize\n",
    "# our list of images and corresponding class labels\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = paths.list_images(path)\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "# loop over our input images\n",
    "for imagePath in imagePaths:\n",
    "\t# load the input image from disk, resize it to 32x32 pixels, scale\n",
    "\t# the pixel intensities to the range [0, 1], and then update our\n",
    "\t# images list\n",
    "\timage = cv2.imread(imagePath)\n",
    "\timage = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "\timage = img_to_array(image)\n",
    "\tdata.append(image)\n",
    "\n",
    "\t# extract the class label from the file path and update the\n",
    "\t# labels list\n",
    "\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\tlabels.append(label)\n",
    "\t\n",
    "\n",
    "# encode the labels, converting them from strings to integers\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "# perform a training and testing split, using 70% of the data for\n",
    "# training and 30% for evaluation\n",
    "(trainX, testX, trainY, testY) = train_test_split(np.array(data),\n",
    "\tnp.array(labels), test_size=0.3, random_state=3)\n",
    "\n",
    "# first set of CONV => RELU => POOL layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(20, (5, 5), padding=\"same\",input_shape=IMAGE_DIMS))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# second set of CONV => RELU => POOL layers\n",
    "model.add(Conv2D(50, (5, 5), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# first (and only) set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation(\"relu\"))\n",
    "# softmax classifier\n",
    "model.add(Dense(count-1))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# train the model using the Adam optimizer\n",
    "print(\"[INFO] training network...\")\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 50)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "\tepochs=20, batch_size=BS)\n",
    "\n",
    "print(\"[INFO] Saving Model...\")\n",
    "filename = model_name\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=lb.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weed_model_dim_32x32.sav\n",
      "3\n",
      "[INFO] loading images...\n",
      "[INFO] training network...\n",
      "Epoch 1/50\n",
      "75/75 [==============================] - 1s 13ms/step - loss: 0.2806 - accuracy: 0.9329 - val_loss: 0.1182 - val_accuracy: 0.9626\n",
      "Epoch 2/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0632 - accuracy: 0.9734 - val_loss: 0.0301 - val_accuracy: 0.9931\n",
      "Epoch 3/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0228 - accuracy: 0.9954 - val_loss: 0.0186 - val_accuracy: 0.9961\n",
      "Epoch 4/50\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.0163 - accuracy: 0.9958 - val_loss: 0.0153 - val_accuracy: 0.9961\n",
      "Epoch 5/50\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.0145 - accuracy: 0.9958 - val_loss: 0.0148 - val_accuracy: 0.9961\n",
      "Epoch 6/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.0123 - val_accuracy: 0.9961\n",
      "Epoch 7/50\n",
      "75/75 [==============================] - 1s 13ms/step - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.0132 - val_accuracy: 0.9961\n",
      "Epoch 8/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 0.0116 - accuracy: 0.9958 - val_loss: 0.0125 - val_accuracy: 0.9961\n",
      "Epoch 9/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0141 - accuracy: 0.9949 - val_loss: 0.0124 - val_accuracy: 0.9961\n",
      "Epoch 10/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0107 - accuracy: 0.9962 - val_loss: 0.0122 - val_accuracy: 0.9961\n",
      "Epoch 11/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0114 - accuracy: 0.9958 - val_loss: 0.0137 - val_accuracy: 0.9961\n",
      "Epoch 12/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0181 - accuracy: 0.9937 - val_loss: 0.0138 - val_accuracy: 0.9961\n",
      "Epoch 13/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0122 - accuracy: 0.9954 - val_loss: 0.0120 - val_accuracy: 0.9961\n",
      "Epoch 14/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0121 - accuracy: 0.9958 - val_loss: 0.0112 - val_accuracy: 0.9970\n",
      "Epoch 15/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0107 - accuracy: 0.9958 - val_loss: 0.0108 - val_accuracy: 0.9961\n",
      "Epoch 16/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0097 - accuracy: 0.9962 - val_loss: 0.0124 - val_accuracy: 0.9961\n",
      "Epoch 17/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0094 - accuracy: 0.9966 - val_loss: 0.0104 - val_accuracy: 0.9970\n",
      "Epoch 18/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0088 - accuracy: 0.9966 - val_loss: 0.0110 - val_accuracy: 0.9970\n",
      "Epoch 19/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0077 - accuracy: 0.9970 - val_loss: 0.0121 - val_accuracy: 0.9970\n",
      "Epoch 20/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0082 - accuracy: 0.9966 - val_loss: 0.0117 - val_accuracy: 0.9970\n",
      "Epoch 21/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0077 - accuracy: 0.9970 - val_loss: 0.0108 - val_accuracy: 0.9951\n",
      "Epoch 22/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0235 - accuracy: 0.9928 - val_loss: 0.0121 - val_accuracy: 0.9970\n",
      "Epoch 23/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0066 - accuracy: 0.9970 - val_loss: 0.0124 - val_accuracy: 0.9970\n",
      "Epoch 24/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0055 - accuracy: 0.9975 - val_loss: 0.0137 - val_accuracy: 0.9970\n",
      "Epoch 25/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.0161 - val_accuracy: 0.9970\n",
      "Epoch 26/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0044 - accuracy: 0.9983 - val_loss: 0.0131 - val_accuracy: 0.9951\n",
      "Epoch 27/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0180 - val_accuracy: 0.9970\n",
      "Epoch 28/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0155 - val_accuracy: 0.9970\n",
      "Epoch 29/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0025 - accuracy: 0.9996 - val_loss: 0.0170 - val_accuracy: 0.9970\n",
      "Epoch 30/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0163 - val_accuracy: 0.9970\n",
      "Epoch 31/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0173 - val_accuracy: 0.9970\n",
      "Epoch 32/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 0.9970\n",
      "Epoch 33/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 0.9970\n",
      "Epoch 34/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 8.7798e-04 - accuracy: 1.0000 - val_loss: 0.0164 - val_accuracy: 0.9970\n",
      "Epoch 35/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 9.7154e-04 - accuracy: 1.0000 - val_loss: 0.0228 - val_accuracy: 0.9970\n",
      "Epoch 36/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 6.8781e-04 - accuracy: 1.0000 - val_loss: 0.0172 - val_accuracy: 0.9961\n",
      "Epoch 37/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0197 - val_accuracy: 0.9970\n",
      "Epoch 38/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 5.0576e-04 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 0.9970\n",
      "Epoch 39/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 3.4571e-04 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9970\n",
      "Epoch 40/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 3.0461e-04 - accuracy: 1.0000 - val_loss: 0.0249 - val_accuracy: 0.9970\n",
      "Epoch 41/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 2.3100e-04 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 0.9970\n",
      "Epoch 42/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 2.7113e-04 - accuracy: 1.0000 - val_loss: 0.0203 - val_accuracy: 0.9970\n",
      "Epoch 43/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 2.7986e-04 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 0.9970\n",
      "Epoch 44/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 1.5240e-04 - accuracy: 1.0000 - val_loss: 0.0264 - val_accuracy: 0.9970\n",
      "Epoch 45/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 1.4049e-04 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 0.9970\n",
      "Epoch 46/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 1.2011e-04 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 0.9970\n",
      "Epoch 47/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 1.0698e-04 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 0.9970\n",
      "Epoch 48/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 9.2309e-05 - accuracy: 1.0000 - val_loss: 0.0264 - val_accuracy: 0.9970\n",
      "Epoch 49/50\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 9.4271e-05 - accuracy: 1.0000 - val_loss: 0.0277 - val_accuracy: 0.9970\n",
      "Epoch 50/50\n",
      "75/75 [==============================] - 1s 12ms/step - loss: 7.9810e-05 - accuracy: 1.0000 - val_loss: 0.0278 - val_accuracy: 0.9970\n",
      "[INFO] Saving Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weed_model_dim_32x32.sav\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: weed_model_dim_32x32.sav\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EACB4F310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EACB4F310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   DaunSakit       1.00      0.25      0.40         4\n",
      "   DaunSehat       0.92      1.00      0.96        34\n",
      "       Tanah       1.00      1.00      1.00       978\n",
      "\n",
      "    accuracy                           1.00      1016\n",
      "   macro avg       0.97      0.75      0.79      1016\n",
      "weighted avg       1.00      1.00      1.00      1016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python basic_cnn.py\n",
    "\n",
    "# import the necessary packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import os.path  \n",
    "import pickle\n",
    "import cv2\n",
    "path = \"D:\\coding\\AI_code\\keras\\melon\\dataset\"\n",
    "BS = 32\n",
    "IMAGE_DIMS = (32, 32 ,3)\n",
    "model_name = 'melon_model_'+'dim_'+str(IMAGE_DIMS[0])+'x'+str(IMAGE_DIMS[1])+'.sav'\n",
    "print(model_name)\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for name in dirs:\n",
    "        \n",
    "        count += 1\n",
    "print(count)\n",
    "\n",
    "\n",
    "# grab all image paths in the input dataset directory, then initialize\n",
    "# our list of images and corresponding class labels\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = paths.list_images(path)\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "# loop over our input images\n",
    "for imagePath in imagePaths:\n",
    "\t# load the input image from disk, resize it to 32x32 pixels, scale\n",
    "\t# the pixel intensities to the range [0, 1], and then update our\n",
    "\t# images list\n",
    "\timage = cv2.imread(imagePath)\n",
    "\timage = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "\timage = img_to_array(image)\n",
    "\tdata.append(image)\n",
    "\n",
    "\t# extract the class label from the file path and update the\n",
    "\t# labels list\n",
    "\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\tlabels.append(label)\n",
    "\t\n",
    "\n",
    "# encode the labels, converting them from strings to integers\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "# perform a training and testing split, using 70% of the data for\n",
    "# training and 30% for evaluation\n",
    "(trainX, testX, trainY, testY) = train_test_split(np.array(data),\n",
    "\tnp.array(labels), test_size=0.3, random_state=3)\n",
    "\n",
    "aug = ImageDataGenerator()\n",
    "# define our Convolutional Neural Network architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(8, (3, 3), padding=\"same\", input_shape=IMAGE_DIMS))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(count))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# train the model using the Adam optimizer\n",
    "print(\"[INFO] training network...\")\n",
    "opt = Adam(learning_rate=1e-3, decay=1e-3 / 50)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "\tepochs=50, batch_size=BS)\n",
    "\n",
    "print(\"[INFO] Saving Model...\")\n",
    "filename = model_name\n",
    "#pickle.dump(model, open(filename, 'wb'))\n",
    "model.save(filename)\n",
    "\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "\tpredictions.argmax(axis=1), target_names=lb.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b72643da4b0eb456850a8cbedce9e5f10f48fa0ef47fc27d4f2224edd5c93091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
